layer_type: 'transformer'
checkpoint_activations: false
encoder_layers: 24
encoder_embed_dim: 1024
encoder_ffn_embed_dim: 4096
encoder_attention_heads: 16
activation_fn: gelu
dropout: 0.1
attention_dropout: 0.1
activation_dropout: 0.0
encoder_layerdrop: 0.05
dropout_input: 0.1
dropout_features: 0.1
final_dim: 256
layer_norm_first: true
logit_temp: 0.1
target_glu: false
conv_pos: 128
conv_pos_groups: 16
required_seq_len_multiple: 2

